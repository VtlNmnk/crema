{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VtlNmnk/crema/blob/master/02_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlyH3VxiMDey",
        "colab_type": "code",
        "outputId": "1cfd0def-74d1-4677-f79f-8648e6d363d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "# Get access to google drive folder.\n",
        "# This only needs to be done once per notebook.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = '/content/gdrive/My Drive/ACE/'  #change dir to your project folder\n",
        "\n",
        "# Change current directoty to mounted on gdrive\n",
        "os.chdir(root_path)\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/ACE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3R8YQd8dbgF",
        "colab_type": "code",
        "outputId": "000ca81f-5411-4c5c-e71e-b39229f84dea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "device_name = tensorflow.test.gpu_device_name()\n",
        "# if device_name != '/device:GPU:0':\n",
        "#   raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sdCy7MHAZfv",
        "colab_type": "code",
        "outputId": "2774cde7-8fea-4d15-bf34-e38709d03876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 14004496103599693379, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12250426452105035657\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 13375306243299948867\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11330115994\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 18293539097563749402\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97z3mLDsPND9",
        "colab_type": "code",
        "outputId": "3ae62ddf-2258-4a15-ee58-1854bbb48a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " augment_results_collection   output_results\n",
            "'Colab Notebooks'\t      output_results_queen\n",
            " crema\t\t\t      output_results_TPU\n",
            " datasets\t\t      pumpp\n",
            " example\t\t      __pycache__\n",
            " libs\t\t\t      pyrubberband\n",
            " log_ACE\t\t      requirements.txt\n",
            " muda\t\t\t      resources\n",
            " output_example_results       rubberband-cli_1.8.1-4_amd64.deb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNsN5WCSKcbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Maximum number of samples to draw per streamer\n",
        "max_samples = 128\n",
        "\n",
        "# Duration (in seconds) of training patches\n",
        "duration = 8.0\n",
        "\n",
        "# Seed for the random number generator\n",
        "seed = 100\n",
        "\n",
        "# Number of active training streamers\n",
        "train_streamers = 1024\n",
        "\n",
        "\n",
        "# Size of training batches\n",
        "batch_size = 16\n",
        "# batch_size = 128\n",
        "\n",
        "# Rate of pescador stream deactivation\n",
        "rate = 16\n",
        "\n",
        "# Maximum number of epochs to train for\n",
        "epochs = 500\n",
        "\n",
        "# Number of batches per epoch\n",
        "epoch_size = 2048\n",
        "# epoch_size = 12\n",
        "\n",
        "# Number of epochs without improvement to stop\n",
        "early_stopping = 50\n",
        "\n",
        "# Number of epochs before reducing learning rate\n",
        "reduce_lr = 20\n",
        "\n",
        "# Path to working directory\n",
        "# working = os.path.join(root_path, \"output_results\")\n",
        "working = \"output_results_queen\"\n",
        "\n",
        "# Path to output directory\n",
        "# output_path = os.path.join(root_path, \"output_results\")\n",
        "output_path = \"output_results_queen/outputs\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A_IS3PVJZIyz",
        "colab": {}
      },
      "source": [
        "def make_sampler(max_samples, duration, pump, seed):\n",
        "    '''stochastic training sampler'''\n",
        "    n_frames = librosa.time_to_frames(duration,\n",
        "                                      sr=pump['cqt'].sr,\n",
        "                                      hop_length=pump['cqt'].hop_length)\n",
        "    return pump.sampler(max_samples, n_frames, random_state=seed)\n",
        "\n",
        "def val_sampler(max_duration, pump, seed):\n",
        "    '''validation sampler'''\n",
        "    n_frames = librosa.time_to_frames(max_duration,\n",
        "                                      sr=pump['cqt'].sr,\n",
        "                                      hop_length=pump['cqt'].hop_length)\n",
        "    return pumpp.sampler.VariableLengthSampler(None, 32, n_frames,\n",
        "                                               *pump.ops,\n",
        "                                               random_state=seed)\n",
        "\n",
        "def data_sampler(fname, sampler):\n",
        "    '''Generate samples from a specified h5 file'''\n",
        "    for datum in sampler(custom_utils.load_h5(fname)):\n",
        "        yield datum\n",
        "\n",
        "def data_generator(working, tracks, sampler, k, augment=True, rate=8, **kwargs):\n",
        "    '''Generate a data stream from a collection of tracks and a sampler'''\n",
        "    seeds = []\n",
        "    for track in tqdm(tracks,\n",
        "             desc='Collecting train data'):\n",
        "        fname = os.path.join(working,\n",
        "                             os.path.extsep.join([track, 'h5']))\n",
        "        seeds.append(pescador.Streamer(data_sampler, fname, sampler))\n",
        "\n",
        "        \n",
        "    # Send it all to a mux\n",
        "    return pescador.StochasticMux(seeds, k, rate, **kwargs)\n",
        "\n",
        "def val_generator(working, tracks, sampler, augment=True):\n",
        "    '''validation generator, deterministic roundrobin'''\n",
        "    seeds = []\n",
        "    for track in tqdm(tracks,\n",
        "             desc='Collecting val data'):\n",
        "        fname = os.path.join(working,\n",
        "                             os.path.extsep.join([track, 'h5']))\n",
        "        seeds.append(pescador.Streamer(data_sampler, fname, sampler))\n",
        "       \n",
        "    # Send it all to a mux\n",
        "    return pescador.RoundRobinMux(seeds)\n",
        "\n",
        "\n",
        "\n",
        "def construct_model(pump):\n",
        "    '''build the model'''\n",
        "    model_inputs = 'cqt/mag'\n",
        "\n",
        "    # Build the input layer\n",
        "    x = pump.layers()[model_inputs]\n",
        "\n",
        "    # Apply batch normalization\n",
        "    x_bn = K.layers.BatchNormalization()(x)\n",
        "\n",
        "    # First convolutional filter: a single 5x5\n",
        "    conv1 = K.layers.Convolution2D(1, (5, 5),\n",
        "                                   padding='same',\n",
        "                                   activation='relu',\n",
        "                                   data_format='channels_last')(x_bn)\n",
        "\n",
        "    c1bn = K.layers.BatchNormalization()(conv1)\n",
        "\n",
        "    # Second convolutional filter: a bank of full-height filters\n",
        "    conv2 = K.layers.Convolution2D(12*6, (1, int(conv1.shape[2])),\n",
        "                                   padding='valid', activation='relu',\n",
        "                                   data_format='channels_last')(c1bn)\n",
        "\n",
        "    c2bn = K.layers.BatchNormalization()(conv2)\n",
        "\n",
        "    # Squeeze out the frequency dimension\n",
        "    squeeze = custom_layers.SqueezeLayer(axis=2)(c2bn)\n",
        "\n",
        "    # BRNN layer\n",
        "    rnn1 = K.layers.Bidirectional(K.layers.GRU(128,\n",
        "                                               return_sequences=True))(squeeze)\n",
        "\n",
        "    r1bn = K.layers.BatchNormalization()(rnn1)\n",
        "\n",
        "    rnn2 = K.layers.Bidirectional(K.layers.GRU(128,\n",
        "                                              return_sequences=True))(r1bn)\n",
        "\n",
        "    # 1: pitch class predictor\n",
        "    pc = K.layers.Dense(pump.fields['chord_struct/pitch'].shape[1],\n",
        "                        activation='sigmoid')\n",
        "\n",
        "    pc_p = K.layers.TimeDistributed(pc, name='chord_pitch')(rnn2)\n",
        "\n",
        "    # 2: root predictor\n",
        "    root = K.layers.Dense(13, activation='softmax')\n",
        "    root_p = K.layers.TimeDistributed(root, name='chord_root')(rnn2)\n",
        "\n",
        "    # 3: bass predictor\n",
        "    bass = K.layers.Dense(13, activation='softmax')\n",
        "    bass_p = K.layers.TimeDistributed(bass, name='chord_bass')(rnn2)\n",
        "\n",
        "    # 4: merge layer\n",
        "    codec = K.layers.concatenate([rnn2, pc_p, root_p, bass_p])\n",
        "\n",
        "    codecbn = K.layers.BatchNormalization()(codec)\n",
        "\n",
        "    p0 = K.layers.Dense(len(pump['chord_tag'].vocabulary()),\n",
        "                        activation='softmax',\n",
        "                        bias_regularizer=K.regularizers.l2())\n",
        "\n",
        "    tag = K.layers.TimeDistributed(p0, name='chord_tag')(codecbn)\n",
        "\n",
        "    model = K.models.Model(x, [tag, pc_p, root_p, bass_p])\n",
        "    model_outputs = ['chord_tag/chord',\n",
        "                     'chord_struct/pitch',\n",
        "                     'chord_struct/root',\n",
        "                     'chord_struct/bass']\n",
        "\n",
        "    return model, [model_inputs], model_outputs\n",
        "\n",
        "def prepare_model(working, max_samples, duration, rate,\n",
        "          batch_size, epochs, epoch_size,\n",
        "          early_stopping, reduce_lr, seed, checkpoint_path=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    working : str\n",
        "        directory that contains the experiment data (h5)\n",
        "    max_samples : int\n",
        "        Maximum number of samples per streamer\n",
        "    duration : float\n",
        "        Duration of training patches\n",
        "    batch_size : int\n",
        "        Size of batches\n",
        "    rate : int\n",
        "        Poisson rate for pescador\n",
        "    epochs : int\n",
        "        Maximum number of epoch\n",
        "    epoch_size : int\n",
        "        Number of batches per epoch\n",
        "    validation_size : int\n",
        "        Number of validation batches\n",
        "    early_stopping : int\n",
        "        Number of epochs before early stopping\n",
        "    reduce_lr : int\n",
        "        Number of epochs before reducing learning rate\n",
        "    seed : int\n",
        "        Random seed\n",
        "    '''\n",
        "    # Load the pump\n",
        "    with open(os.path.join(working, 'pump.pkl'), 'rb') as fd:\n",
        "        pump = pickle.load(fd)\n",
        "    # Build the sampler\n",
        "    sampler = make_sampler(max_samples, duration, pump, seed)\n",
        "    # And the validation sampler: cap out at 10 minutes\n",
        "    # sampler_val = val_sampler(10 * 60, pump, seed)#too slow\n",
        "\n",
        "    sampler_val = val_sampler(10, pump, seed)#test\n",
        "    # Build the model\n",
        "    model, inputs, outputs = construct_model(pump)\n",
        "    # Load the training data\n",
        "    idx_train_ = pd.read_json(os.path.join(working, 'index_train.json'))\n",
        "    # Split the training data into train and validation\n",
        "    splitter_tv = ShuffleSplit(n_splits=1, test_size=0.25,\n",
        "                               random_state=seed)\n",
        "    train, val = next(splitter_tv.split(idx_train_))\n",
        "    idx_train = idx_train_.iloc[train]\n",
        "    idx_val = idx_train_.iloc[val]\n",
        "    gen_train = data_generator(working,\n",
        "                               idx_train['id'].values, sampler, epoch_size,\n",
        "                               augment=True,\n",
        "                               rate=rate,\n",
        "                               mode='with_replacement',\n",
        "                               random_state=seed)\n",
        "    \n",
        "    gen_train = pescador.maps.keras_tuples(pescador.maps.buffer_stream(gen_train(),\n",
        "                                                                       batch_size,\n",
        "                                                                       axis=0),\n",
        "                                           inputs=inputs,\n",
        "                                           outputs=outputs)\n",
        "    gen_val = val_generator(working,\n",
        "                            idx_val['id'].values, sampler_val,\n",
        "                            augment=True)\n",
        "    validation_size = gen_val.n_streams\n",
        "    gen_val = pescador.maps.keras_tuples(gen_val(),\n",
        "                                         inputs=inputs,\n",
        "                                         outputs=outputs)\n",
        "\n",
        "    loss = {'chord_tag': 'sparse_categorical_crossentropy'}\n",
        "    metrics = {'chord_tag': 'sparse_categorical_accuracy'}\n",
        "    loss.update(chord_pitch='binary_crossentropy',\n",
        "                chord_root='sparse_categorical_crossentropy',\n",
        "                chord_bass='sparse_categorical_crossentropy')\n",
        "    # monitor = 'val_chord_tag_sparse_categorical_accuracy'\n",
        "    monitor = 'val_chord_tag_loss'\n",
        "    # sgd = K.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    adam = K.optimizers.Adam(lr=0.1e-5)\n",
        "\n",
        "    model.compile(optimizer=adam, loss=loss, metrics=metrics)\n",
        "    # model.compile(sgd, loss=loss, metrics=metrics)\n",
        "\n",
        "    # Store the model\n",
        "    model_spec = K.utils.serialize_keras_object(model)\n",
        "    with open(os.path.join(output_path, 'model_spec.pkl'), 'wb') as fd:\n",
        "        pickle.dump(model_spec, fd)\n",
        "\n",
        "    return model, monitor, gen_train, gen_val, validation_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSOtlOy65Txs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !dpkg -i rubberband-cli_1.8.1-4_amd64.deb\n",
        "# !pip install jams\n",
        "# !pip install soundfile\n",
        "# !pip install jsonpickle\n",
        "# !pip install pescador\n",
        "# !pip uninstall pumpp\n",
        "# !pip uninstall keras\n",
        "# !pip install crema\n",
        "\n",
        "# !pip uninstall crema"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ1lAeMgJMT1",
        "colab_type": "code",
        "outputId": "f68d34e6-32fa-4ccc-9a31-204538028258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "import pickle\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "import glob\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "!pip install keras==2.1.2\n",
        "# !pip install keras\n",
        "# import keras as K\n",
        "\n",
        "\n",
        "try:\n",
        "    import keras as K\n",
        "except ImportError:\n",
        "    !pip install -q keras==2.1.2\n",
        "    import keras as K\n",
        "\n",
        "try:\n",
        "    import pescador\n",
        "except ImportError:\n",
        "    !pip install -q pescador\n",
        "    import pescador\n",
        "\n",
        "try:\n",
        "    import jams\n",
        "    from jams.util import smkdirs\n",
        "except ImportError:\n",
        "    !pip install -q jams\n",
        "    import jams\n",
        "    from jams.util import smkdirs\n",
        "\n",
        "try:\n",
        "    import librosa\n",
        "except ImportError:\n",
        "    # !pip install -q librosa==0.6.2\n",
        "    !pip install -q librosa\n",
        "    import librosa\n",
        "\n",
        "try:\n",
        "    import muda\n",
        "except ImportError:\n",
        "    !pip install -q muda\n",
        "    import muda\n",
        "\n",
        "import pumpp\n",
        "from crema import layers as custom_layers\n",
        "from crema import utils as custom_utils\n",
        "\n",
        "# checkpoint_path = os.path.join(output_path, 'checkpoint_model.h5')\n",
        "\n",
        "smkdirs(output_path)\n",
        "\n",
        "#version = crema_custom.utils.increment_version(os.path.join(output_path, 'version.txt'))\n",
        "model, monitor, gen_train, gen_val, validation_size = prepare_model(working,\n",
        "                                                max_samples, duration,\n",
        "                                                rate, batch_size,\n",
        "                                                epochs, epoch_size,\n",
        "                                                early_stopping,\n",
        "                                                reduce_lr, seed, \n",
        "                                                # checkpoint_path=os.path.join(output_path, 'saved_models/weights-improvement-ep20-acc0.66-chord_loss1.34.hdf5')\n",
        "                                                                        )\n",
        "\n",
        "# Construct the checkpoint path\n",
        "\n",
        "checkpoint_path = os.path.join(output_path, \"saved_models/weights-improvement-ep{epoch:02d}-acc{val_chord_tag_sparse_categorical_accuracy:.2f}-chord_loss{val_chord_tag_loss:.2f}.hdf5\")\n",
        "# checkpoint = ModelCheckpoint(filepath, monitor=monitor, verbose=1, save_best_only=True, mode='max')\n",
        "# callbacks_list = [checkpoint]\n",
        "\n",
        "# print(\"checkpoint_path is {}\".format(checkpoint_path))\n",
        "# Build the callbacks\n",
        "cb = []\n",
        "\n",
        "class CustomSaver(K.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.model.save(\"output_results/saved_models/model_{}.hd5\".format(epoch+1))\n",
        "saver = CustomSaver()\n",
        "cb.append(saver)\n",
        "\n",
        "cb.append(K.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                        save_best_only=True,\n",
        "                                        verbose=1,\n",
        "                                        monitor=monitor))\n",
        "\n",
        "cb.append(K.callbacks.ReduceLROnPlateau(patience=reduce_lr,\n",
        "                                        verbose=1,\n",
        "                                        monitor=monitor))\n",
        "\n",
        "cb.append(K.callbacks.EarlyStopping(patience=early_stopping,\n",
        "                                    verbose=1,\n",
        "                                    monitor=monitor))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.1.2 in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.12.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0903 11:13:39.145034 140517761030016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0903 11:13:39.155191 140517761030016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0903 11:13:39.208545 140517761030016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0903 11:13:39.669530 140517761030016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "Collecting train data: 100%|██████████| 450/450 [00:00<00:00, 119442.91it/s]\n",
            "Collecting val data: 100%|██████████| 150/150 [00:00<00:00, 88612.06it/s]\n",
            "W0903 11:13:40.801546 140517761030016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0903 11:13:40.812002 140517761030016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2915: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0903 11:13:40.838841 140517761030016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0903 11:13:40.900653 140517761030016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M457KBPbR1N7",
        "colab_type": "code",
        "outputId": "895678db-ea41-43d7-9533-e9467f32c7c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "# Fit the model\n",
        "history = model.fit_generator(gen_train, epoch_size, epochs,\n",
        "                    validation_data=gen_val,\n",
        "                    validation_steps=validation_size,\n",
        "                    callbacks=cb,\n",
        "                    # initial_epoch=20\n",
        "                    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0903 11:13:51.757548 140517761030016 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0903 11:13:51.772784 140517761030016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "   2/2048 [..............................] - ETA: 13:56:28 - loss: 11.8369 - chord_tag_loss: 5.5940 - chord_pitch_loss: 0.7499 - chord_root_loss: 2.7720 - chord_bass_loss: 2.7210 - chord_tag_sparse_categorical_accuracy: 0.0051"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:116: UserWarning: Method on_batch_end() is slow compared to the batch update (1.145345). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   3/2048 [..............................] - ETA: 9:41:35 - loss: 11.9034 - chord_tag_loss: 5.6185 - chord_pitch_loss: 0.7525 - chord_root_loss: 2.7821 - chord_bass_loss: 2.7503 - chord_tag_sparse_categorical_accuracy: 0.0065 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:116: UserWarning: Method on_batch_end() is slow compared to the batch update (0.958461). Check your callbacks.\n",
            "  % delta_t_median)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2047/2048 [============================>.] - ETA: 0s - loss: 11.5557 - chord_tag_loss: 5.5824 - chord_pitch_loss: 0.7462 - chord_root_loss: 2.6355 - chord_bass_loss: 2.5916 - chord_tag_sparse_categorical_accuracy: 0.0096Epoch 00001: val_chord_tag_loss improved from inf to 5.24637, saving model to output_results_queen/outputs/saved_models/weights-improvement-ep01-acc0.02-chord_loss5.25.hdf5\n",
            "2048/2048 [==============================] - 1301s 635ms/step - loss: 11.5554 - chord_tag_loss: 5.5823 - chord_pitch_loss: 0.7462 - chord_root_loss: 2.6354 - chord_bass_loss: 2.5915 - chord_tag_sparse_categorical_accuracy: 0.0096 - val_loss: 10.5202 - val_chord_tag_loss: 5.2464 - val_chord_pitch_loss: 0.7240 - val_chord_root_loss: 2.3049 - val_chord_bass_loss: 2.2449 - val_chord_tag_sparse_categorical_accuracy: 0.0176\n",
            "Epoch 2/500\n",
            "2047/2048 [============================>.] - ETA: 0s - loss: 10.7533 - chord_tag_loss: 5.2920 - chord_pitch_loss: 0.7301 - chord_root_loss: 2.3821 - chord_bass_loss: 2.3491 - chord_tag_sparse_categorical_accuracy: 0.0792Epoch 00002: val_chord_tag_loss improved from 5.24637 to 5.05965, saving model to output_results_queen/outputs/saved_models/weights-improvement-ep02-acc0.20-chord_loss5.06.hdf5\n",
            "2048/2048 [==============================] - 890s 434ms/step - loss: 10.7528 - chord_tag_loss: 5.2918 - chord_pitch_loss: 0.7301 - chord_root_loss: 2.3820 - chord_bass_loss: 2.3490 - chord_tag_sparse_categorical_accuracy: 0.0793 - val_loss: 10.0338 - val_chord_tag_loss: 5.0596 - val_chord_pitch_loss: 0.7216 - val_chord_root_loss: 2.1149 - val_chord_bass_loss: 2.1376 - val_chord_tag_sparse_categorical_accuracy: 0.1974\n",
            "Epoch 3/500\n",
            "2047/2048 [============================>.] - ETA: 0s - loss: 10.0509 - chord_tag_loss: 5.0722 - chord_pitch_loss: 0.7084 - chord_root_loss: 2.1425 - chord_bass_loss: 2.1277 - chord_tag_sparse_categorical_accuracy: 0.2154Epoch 00003: val_chord_tag_loss did not improve\n",
            "2048/2048 [==============================] - 898s 438ms/step - loss: 10.0503 - chord_tag_loss: 5.0718 - chord_pitch_loss: 0.7084 - chord_root_loss: 2.1424 - chord_bass_loss: 2.1276 - chord_tag_sparse_categorical_accuracy: 0.2155 - val_loss: 10.1170 - val_chord_tag_loss: 5.3383 - val_chord_pitch_loss: 0.7459 - val_chord_root_loss: 2.0288 - val_chord_bass_loss: 2.0040 - val_chord_tag_sparse_categorical_accuracy: 0.2841\n",
            "Epoch 4/500\n",
            "2047/2048 [============================>.] - ETA: 0s - loss: 9.4760 - chord_tag_loss: 4.9590 - chord_pitch_loss: 0.6827 - chord_root_loss: 1.9216 - chord_bass_loss: 1.9126 - chord_tag_sparse_categorical_accuracy: 0.2622Epoch 00004: val_chord_tag_loss did not improve\n",
            "2048/2048 [==============================] - 887s 433ms/step - loss: 9.4757 - chord_tag_loss: 4.9589 - chord_pitch_loss: 0.6827 - chord_root_loss: 1.9215 - chord_bass_loss: 1.9126 - chord_tag_sparse_categorical_accuracy: 0.2622 - val_loss: 9.1982 - val_chord_tag_loss: 5.1229 - val_chord_pitch_loss: 0.6624 - val_chord_root_loss: 1.7233 - val_chord_bass_loss: 1.6895 - val_chord_tag_sparse_categorical_accuracy: 0.3291\n",
            "Epoch 5/500\n",
            "2047/2048 [============================>.] - ETA: 0s - loss: 8.9911 - chord_tag_loss: 4.8198 - chord_pitch_loss: 0.6485 - chord_root_loss: 1.7610 - chord_bass_loss: 1.7616 - chord_tag_sparse_categorical_accuracy: 0.2949Epoch 00005: val_chord_tag_loss improved from 5.05965 to 4.76871, saving model to output_results_queen/outputs/saved_models/weights-improvement-ep05-acc0.38-chord_loss4.77.hdf5\n",
            "2048/2048 [==============================] - 884s 432ms/step - loss: 8.9912 - chord_tag_loss: 4.8196 - chord_pitch_loss: 0.6485 - chord_root_loss: 1.7612 - chord_bass_loss: 1.7618 - chord_tag_sparse_categorical_accuracy: 0.2948 - val_loss: 8.3352 - val_chord_tag_loss: 4.7687 - val_chord_pitch_loss: 0.6446 - val_chord_root_loss: 1.4597 - val_chord_bass_loss: 1.4621 - val_chord_tag_sparse_categorical_accuracy: 0.3770\n",
            "Epoch 6/500\n",
            "2047/2048 [============================>.] - ETA: 0s - loss: 8.5872 - chord_tag_loss: 4.7055 - chord_pitch_loss: 0.6078 - chord_root_loss: 1.6344 - chord_bass_loss: 1.6393 - chord_tag_sparse_categorical_accuracy: 0.3196Epoch 00006: val_chord_tag_loss improved from 4.76871 to 4.54456, saving model to output_results_queen/outputs/saved_models/weights-improvement-ep06-acc0.37-chord_loss4.54.hdf5\n",
            "2048/2048 [==============================] - 888s 434ms/step - loss: 8.5870 - chord_tag_loss: 4.7056 - chord_pitch_loss: 0.6078 - chord_root_loss: 1.6342 - chord_bass_loss: 1.6392 - chord_tag_sparse_categorical_accuracy: 0.3196 - val_loss: 8.0004 - val_chord_tag_loss: 4.5446 - val_chord_pitch_loss: 0.5716 - val_chord_root_loss: 1.4364 - val_chord_bass_loss: 1.4476 - val_chord_tag_sparse_categorical_accuracy: 0.3746\n",
            "Epoch 7/500\n",
            "2047/2048 [============================>.] - ETA: 0s - loss: 8.2927 - chord_tag_loss: 4.6415 - chord_pitch_loss: 0.5678 - chord_root_loss: 1.5400 - chord_bass_loss: 1.5431 - chord_tag_sparse_categorical_accuracy: 0.3349Epoch 00007: val_chord_tag_loss did not improve\n",
            "2048/2048 [==============================] - 893s 436ms/step - loss: 8.2922 - chord_tag_loss: 4.6414 - chord_pitch_loss: 0.5678 - chord_root_loss: 1.5398 - chord_bass_loss: 1.5430 - chord_tag_sparse_categorical_accuracy: 0.3349 - val_loss: 7.8937 - val_chord_tag_loss: 4.6532 - val_chord_pitch_loss: 0.5324 - val_chord_root_loss: 1.3505 - val_chord_bass_loss: 1.3573 - val_chord_tag_sparse_categorical_accuracy: 0.3988\n",
            "Epoch 8/500\n",
            "1852/2048 [==========================>...] - ETA: 1:24 - loss: 8.0592 - chord_tag_loss: 4.5111 - chord_pitch_loss: 0.5338 - chord_root_loss: 1.5027 - chord_bass_loss: 1.5113 - chord_tag_sparse_categorical_accuracy: 0.3498"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}